# SSAFY_Daliy_Project

## 200320|Day10|특화 프로젝트

### Word Vector Representations (Word Embedding)

- one-hot encoding 의 한계로 나온 방법

- one-hot encoding

  - 어떤 문장에 있는 단어 전부를 유사도와 관계없이 각각의 단어를 벡터로 표현하는 인코딩 방식

- 단어 한 개에 대해 여러개의 벡터로 표현

- 한 개의 단어가 실수 여러개로 표현가능

- 비슷한 유사도를 갖고 있으면 벡터 차원에서 위치하는 방식이 같음

- Word Similarity Task

  - 어떤 두 항목 a, b 에 대하여 유사도를 판단 가능

  - $$
    cos(a,b) = \frac {a \circ b}{\lVert a \rVert	\lVert b \rVert}
    $$

    

​		

### BERT(Bidirectional Encoder Representations from Transformers)

- 구글이 낸 새로운 language representational model 
- Token / Segment / Position 임베딩 의 합으로 구성
  - Token Embeddings
    - Word Piece 임베딩 방식을 사용
    - 이 방법은 문장에서 자주 등장하면서 가장 긴 길이의 sub-word를 하나의 단위로 만듦
    - 자주 등장하지 않는 단어는 sub-word로 나눔
  - Sentence  Embeddings
    - bert 는 두 개의 문장을 문자 구분자와 함께 합침
    - 두 문장의 길이 제한은 512 sub-words
  - Position Embedding
    - self-attention 모델을 사용 
- 이 3가지를 합해서 하나의 임베딩으로 만듦





### 느낀점

각각의 단어의 유사도를 비교하여 임베딩하는 방법인 Word Vector Representations의 방법과 단어의 빈도 수 와 길이 등을 이용하여 분석하는 BERT방식을 비교하면서 두 개념의 차이점을 명확히 배울 수 있었습니다.